# üìÅ VectorInsight: Multi-File PDF/TXT/Q&A with LLaMA 3


VectorInsight enables you to upload and semantically search across multiple PDF, TXT, and ZIP documents, leveraging the power of LLaMA 3 and vector search for rich, context-aware Q&A.

---

## üöÄ Features

- **Multi-File Support:** Upload and process multiple PDF, TXT, and ZIP files (ZIP can contain PDFs or TXTs).
- **Semantic Q&A:** Ask questions about all uploaded documents and get concise, contextually accurate answers powered by LLaMA 3.
- **Vector Search:** Embeds and indexes document chunks with `sentence-transformers` and FAISS for fast, relevant retrieval.
- **Modern UI:** Drag-and-drop upload, clear question input, and answer display.
- **Secure API Key Management:** Uses Streamlit secrets for Groq API key.

---

## üñ•Ô∏è Interface Overview

- **Upload Area:** Drag and drop or browse for PDF, TXT, or ZIP files (limit 200MB per file).
- **Question Box:** Enter your question about the uploaded documents.
- **Results:** Get answers generated by LLaMA 3 and see the most relevant source document chunks.

![App Screenshot](image1)

---

## üõ†Ô∏è Installation

1. **Clone the repository**
    ```bash
    git clone https://github.com/yourusername/yourrepo.git
    cd yourrepo
    ```

2. **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```

3. **Set up Groq API key**
    - Create a `.streamlit/secrets.toml` file:
      ```toml
      GROQ_API_KEY = "your_groq_api_key_here"
      ```

---

## ‚ö° Usage

1. **Start the app:**
    ```bash
    streamlit run app.py
    ```

2. **In your browser:**
    - Upload .pdf, .txt, or .zip files (multiple at once).
    - Type your question in the input box.
    - Get a LLaMA 3 powered answer and view the top matching document chunks.

---

## üì¶ Supported File Types

- PDF (`.pdf`)
- Text (`.txt`)
- ZIP archives containing PDF/TXT files (`.zip`)

---

## ü§ñ How It Works

1. **Upload:** Select or drag and drop multiple files.
2. **Extract:** Text is extracted from supported file types (including inside ZIPs).
3. **Chunk:** Text is split into overlapping chunks using LangChain's splitter.
4. **Embed:** Each chunk is embedded with a SentenceTransformer model.
5. **Index:** FAISS indexes the vectors for semantic similarity search.
6. **Ask:** User question is embedded; top relevant chunks are retrieved.
7. **Answer:** Chunks and question are sent to LLaMA 3 via Groq API for a context-aware answer.

---

## üì∑ Screenshot

![Screenshot 2025-06-28 182040](https://github.com/user-attachments/assets/a100a9f5-7eed-4c90-af72-29f4b1953f67)


## üôè Acknowledgements

- [Streamlit](https://streamlit.io/)
- [PyMuPDF](https://pymupdf.readthedocs.io/)
- [sentence-transformers](https://www.sbert.net/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Groq](https://groq.com/)
- [LLaMA 3](https://ai.meta.com/llama/)

---

